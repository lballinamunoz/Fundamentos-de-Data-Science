---
title: 'Fundamentos de Data Science: PEC1 - Topic Modeling'
author: "UOC - Master Business Intelligence y Big Data"
date: "Noviembre del 2022"
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: B0.477-PEC-header.html
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
---

******

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



******
# Introducción
******
## Descripción de la PEC a realizar
La prueba está estructurada en 8 ejercicios teórico-prácticos que piden que se desarrolle la fase de preparación y estimación de un modelo utilizando un juego de datos.  

Deben responderse al menos 7 de los 8 ejercicios para poder superar la PEC. Para optar a la máxima nota tienen que responderse los 8 ejercicios.  

## Criterios de evaluación
**Ejercicios teóricos**  
Todos los ejercicios deben ser presentados de forma razonada y clara. No se aceptará ninguna respuesta que no esté claramente justificada.  

**Ejercicios prácticos**  
Para todas las PEC es necesario documentar en cada ejercicio práctico qué se ha hecho y cómo se ha hecho.  

Pregunta  | Criterio de valoración| Peso
---- | ------------- | ----
1  | Respuesta a la pregunta de forma correcta | 5%
2  | Primera visualización | 5%
2  | Segunda visualización | 5%
3  | Se realiza el gráfico solicitado| 10%
3  | Se contestan las cuestiones y se justifican | 5%
4  | Se describen los resultados que se solicitan | 15%
5  | Se contesta a la pregunta planteada | 10%
6  | Se entrena el modelo solicitado |15%
6  | Se describe el proceso solicitado  | 10%
7  | Se contesta a la cuestión planteada | 10%
8  | Se contesta a la cuestión planteada | 10%

## Formato y fecha de entega
El formato de entrega es: studentname-PECn.html  
Fecha de Entrega: 27/11/2022  
Se debe entregar la PEC en el buzón de entregas del aula  

******
# Base teórica
******
Esta práctica se basa en una de las aplicaciones de la **Minería de Textos**, que consiste en clasificar documentos en función de su temática. Esto es lo que se conoce como **Topic Model**. Para realizar la clasificación utilizaremos el algoritmo de aprendizaje automático K-Nearest Neighbors (K-NN).  

Por Topic Model entendemos procesos de aprendizaje automático que tienen por objetivo descubrir el tema subyacente en una colección de documentos. Generalizando un poco más, Topic Model busca patrones en el contenido de los documentos y lo hace en base a la frecuencia de aparición de palabras.  

El análisis que realizamos se basa en el hecho de que en documentos pertenecientes a un mismo tema aparecerán palabras que se repetirán con mayor frecuencia. Por lo tanto, el análisis plantea la clasificación de documentos utilizando como criterio las palabras que son más frecuentes en cada temática. Estas frecuencias se almacenarán en una matriz de datos, donde las variables serán las palabras y los registros los documentos, que será la base para que puedan trabajar los algoritmos de aprendizaje automático, que en esta práctica se centran en el K-NN.  

En este ámbito de conocimiento se basan los sistemas de clasificación documental, búsqueda de contenidos y sistemas de recomendación entre otros.

## Competencias
Las competencias que se trabajan en esta práctica son:  

* Capacidad de analizar datos textuales en R.  
* Capacidad de implementar el algoritmo K-NN para la clasificación de textos según temática.  

## Objetivos
* Asimilar correctamente los apartados 1.2.1, 1.2.2, 2.1, 3.2.1 y 3.2.2.   

## Recursos
Para realizar esta práctica recomendamos la lectura de lo siguiente:  

* Apartados 1.2.1, 1.2.2, 2.1, 3.2.1 y 3.2.2  del material didático **Análisis de datos para organizaciones**.  

* Los siguientes **Recursos en la web:**  

  -- [Journal Digital: Topic Modeling](http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/)  
  -- [Wikipedia: Topic Modeling](http://en.wikipedia.org/wiki/Topic_model)  
  -- [Wikipedia: Sistemas de recomendación](http://es.wikipedia.org/wiki/Sistema_de_recomendaci%C3%B3n)  
  -- [CRAN: Text Mining Package](http://cran.r-project.org/web/packages/tm/tm.pdf)  


## Nota: Propiedad intelectual 

> A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre y cuando esto se documente claramente y no suponga plagio en la práctica. 

> Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se debe presentar junto con ella un documento en el que se detallan todos ellos, especificando el nombre de cada recurso, su autor, el lugar dónde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...). 
El estudiante deberá asegurarse de que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright. 

> Deberéis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.  

******
# Enunciado  
******
El objetivo es clasificar un conjunto de artículos de Reuters correspondientes a distintas temáticas: acquire,
crude, earn, grain, interest, money-fx, ship y trade. Se trata de temáticas relacionadas con inversiones financieras y fondos de inversión.  

Se utiliza un sistema de clasificación de documentos que se basa en el algoritmo de aprendizaje automático K-NN (K-Nearest Neighbors o K vecinos más próximos).    

Los datos están en el fichero data_reuter.txt. Este fichero contiene dos campos, el primero se corresponde con el tipo de temática (en total hay 8) y el segundo campo contiene el artículo relacionado. Entre las 8 temáticas se seleccionan 2 para realizar el análisis.

Para mostrar el funcionamiento del algoritmo de clasificación se utiliza un 70% de los artículos para entrenar el modelo de aprendizaje. Dicho algoritmo se aplica sobre el 30% de artículos restantes con el objetivo de predecir su temática.

******
# Apartados de la práctica
******
El código R que utilizaremos en la práctica se divide en apartados según las tareas que iremos realizando:  

* Lectura y selección de los datos  
* Creación del corpus, limpieza y acondicionado del texto 
* Generación de la Matriz de Términos (TDM-Terms Data Matrix).  
* Creación de la TDM.  
     + Subselecciones sobre TDM.  
* Descripción de la TDM.
* Creación de un data.frame apto para K-NN.  
* Construcción del Modelo de Clasificación con la función `knn()` del paquete "class" y con la función `train()` del paquete "caret".  
* Validación del Modelo de Clasificación.  

******
# Inicialización de variables
******
Instalamos los packages de R que necesitaremos para realizar la práctica:

* install.packages("tm")
* install.packages("plyr")
* install.packages("class")
* install.packages("ggplot2")
* install.packages("SnowballC")
* install.packages("wordcloud")
* install.packages("Rcpp")
* install.packages("caret")


Definimos el directorio de trabajo donde tendremos guardado el fichero de datos.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
setwd("~/Personal/Ms_BI_and_BDAnalytics/Data Science/PEC1_TextMining")
```

Para instalar los paquetes y definir el directorio de trabajo podéis eliminar los asteriscos delante de los **install.packages()** y **setwd()** seleccionarlos y ejecutarlos como comandos de R (Control+Intro).

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
## Cargamos los paquetes necesarios para ejecutar las funciones que se describen a continuación:
# Para la función Corpus()
library(tm)
# Para la función rbind.fill
library(plyr)
# Para la función knn()
library(class)

# En R una variable tipo factor es una variable categórica que puede contener tanto números como carácteres. Se trata de un tipo de variable muy útil para realizar tareas de modelización estadística.

# En R, por defecto, las columnas con carácteres no numéricos son tratadas como factores. Para evitarlo y garantizar que estas columnas sigan siendo consideradas carácteres, fijaremos el siguiente parámetro
options(stringsAsFactors = FALSE)

# Leemos los datos
data <- read.table('data_reuter.txt', header=FALSE, sep='\t')
# Describimos los datos
## Cuantos hay en total
nrow(data)
# Cuantos hay para cada tipo de temática
table(data$V1)

library(ggplot2)
qplot(data$V1,xlab="Tematica", main = "Frecuencias")+ coord_flip()

# Finalmente seleccionamos dos temáticas: acq y earn, que como podemos ver en el gráfico de frecuencias, son las más representativas
data2<-data[which(data$V1 %in% c("acq","earn")),]
## Cuantos hay en total
nrow(data2)
```

******
# Creación del corpus, limpieza y acondicionado del texto.
******
A continuación creamos un corpus para cada temática sobre los que se realizarán las siguietes tareas de **acondicionado de texto**:  

1. Eliminar signos de puntuación.  
2. Eliminar espacios en blanco innecesarios.  
3. Convertir todo el texto a minúsculas.  
4. Eliminar palabras sin significado propio.  
5. Eliminar números.  
6. Substituir las palabras derivadas por su palabra raíz.  

Para la creación de un corpus basta con crear un vector source (vector que interpreta cada elemento que contiene como un documento), para posteriormente formar el corpus (colecciones de documentos que contienen texto) a partir de este vector source.
```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Creación del corpus de la temática acq
## Seleccionamos la temática
data_acq<-data2[(data2$V1=="acq"),]
## Construimos el corpus
### A vector source interprets each element of the vector data_acq$V2 as a document.
?VectorSource
source <- VectorSource(data_acq$V2)
### Corpora are collections of documents containing (natural language) text.
?Corpus
corpus1 <- Corpus(source)
## Acondicionamos el corpus
?tm_map
### Interface to apply transformation functions (also denoted as mappings) to corpora.
### Convertir todo el texto a minúsculas
corpus1 <- tm_map(corpus1, content_transformer(tolower))
### Elimina números
corpus1 <- tm_map(corpus1, removeNumbers)
### Eliminar signos de puntuación
corpus1 <- tm_map(corpus1, removePunctuation)
###Eliminar espacios en blanco innecesarios
corpus1 <- tm_map(corpus1, stripWhitespace)
### Eliminar palabras sin significado propio
v_stopwords <- c(stopwords("english"),c("dont","didnt","arent","cant","one","also","said"))
corpus1 <- tm_map(corpus1, removeWords, v_stopwords)
### Eliminar signos de puntuación
corpus1 <- tm_map(corpus1, removePunctuation)
### Substituir las palabras derivadas por su palabra raíz
corpus1 <- tm_map(corpus1, stemDocument, language="english")

# Creación del corpus de la temática earn
## Seleccionamos la temática
data_earn<-data2[(data2$V1=="earn"),]
## Construimos el corpus
source <- VectorSource(data_earn$V2)
corpus2 <- Corpus(source)
## Acondicionamos el corpus
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, stripWhitespace)
v_stopwords <- c(stopwords("english"),c("dont","didnt","arent","cant","one","also","said"))
corpus2 <- tm_map(corpus2, removeWords, v_stopwords)
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, stemDocument, language="english")
```
******
# Generación de la Matriz de Términos (TDM-Terms Data Matrix)
******
Seguidamente construimos una matriz de términos para cada temática para posteriormete unirlas en una misma lista.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Construimos la matrix de documentos de la temática acq
?TermDocumentMatrix
mat_acq <- TermDocumentMatrix(corpus1)
mat_acq
## Controlamos la dispersión (Sparsity): Número de celdas igual a cero respecto al total.
?removeSparseTerms
mat_acq<- removeSparseTerms(mat_acq,  0.80)
### Para poder ver la matriz utilizamos inspect
inspect(mat_acq)
### Crea una lista: el primer elemento es name y el segundo es la matriz mat_acq, a la que llama mat
?list
mat_acq<-list(name="acq",mat=mat_acq)
mat_acq
str(mat_acq)

# Construimos la matrix de documentos de la temática earn
mat_earn <- TermDocumentMatrix(corpus2)
mat_earn<- removeSparseTerms(mat_earn,  0.80)
inspect(mat_earn)
mat_earn<-list(name="earn",mat=mat_earn)
mat_earn
str(mat_earn)

# Juntamos ambas matrices de términos en una misma lista
mat<-list(mat_acq, mat_earn)
str(mat)
```


******
## Visualizaciones sobre la matriz de palabras TDM  
******  
Con las dos matrices de frecuencias `mat[[1]]$mat` para acq y `mat[[2]]$mat` para earn, podemos realizar algunas visualizaciones básicas.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Frecuencia de los 10 primeros términos en los 10 primeros documentos para ambos temas
inspect(mat[[1]]$mat[1:10,1:10])
inspect(mat[[2]]$mat[1:10,1:10])
# Frecuencia de los 10 primeros términos en todos los documentos del tema acq
inspect(mat[[1]]$mat[1:10,])
# Frecuencia de los términos en los documentos del tema earn
inspect(mat[[2]]$mat)
# Inventario de los primeros términos del del tema earn
head(mat[[2]]$mat$dimnames$Terms)
# Número de documentos del tema acq
nDocs(mat[[1]]$mat)
# Número de términos del tema acq
nTerms(mat[[1]]$mat)
# Visualizamos los términos con más de 100 apariciones en documentos de temática acq
findFreqTerms(mat[[1]]$mat, lowfreq=100)
```

******
# Descripción de la TDM.
******

******
## Representación gráfica de las frecuencias
******

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Para acq
mmat_acq <- as.matrix(mat[[1]]$mat)
# Agregamos las frecuencias por términos y las ordenamos de mayor a menor
v_acq <- sort(rowSums(mmat_acq), decreasing=TRUE)
# Creamos un data.frame con términos y frecuencias
d_acq <- data.frame(word=names(v_acq), freq=v_acq)
d_acq[,3]<-"acq"
# Hacemos lo mismo para earn
mmat_earn <- as.matrix(mat[[2]]$mat)
# Agregamos las frecuencias por términos y las ordenamos de mayor a menor  
v_earn <- sort(rowSums(mmat_earn), decreasing=TRUE)
# Creamos un data.frame con términos y frecuencias
d_earn <- data.frame(word=names(v_earn), freq=v_earn)
d_earn[,3]<-"earn"

# Concatenamos las dos matrices
fdata<-rbind(d_acq,d_earn)
colnames(fdata)
colnames(fdata)<-c("Palabra", "Frecuencia", "Tematica")

# Gráfico de barras con las palabras más frecuentes
library(ggplot2)

ggplot(subset(fdata, Frecuencia>500),aes(Palabra,Frecuencia,fill=Tematica))+geom_bar(stat="identity",position=position_dodge())+theme(axis.text.x=element_text(angle=45, hjust=1))
```

******
## Construcción de una nube de palabras
******
Podemos construir una nube de palabras para la matriz de términos con ambas temáticas. 
```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Cargamos la librería wordcloud
require(wordcloud)
# Construimos la nube de palabras o términos, para ello primero seleccionamos los que tienen una frecuencia superior a 500
sfdata<-subset(fdata, Frecuencia>500)
wordcloud(sfdata$Palabra, fdata$Frecuencia,min.freq=500,random.color=FALSE, colors=rainbow(3))

```

******
# Creación de un data.frame apto para K-NN
******
A continuación se construirá un data.frame en el que las columnas representan Términos, las filas Documentos y las celdas Frecuencias del término o palabra en cada documento.  


```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Creación de un data.frame apto para K-NN
# Para acq
# La función t transpone la matriz o el df dado
?t
s.mat_acq <- t(data.matrix(mat[[1]]$mat))
# La convertimos en data.frame que vendría a ser como un formato excel (filas, columnas y celdas con valores)  
# En este data.frame, tenemos que cada fila es un documento, cada columna una palabra y las celdas contienen la frecuencia en que cada palabra aparece en cada documento.
s.df_acq <- as.data.frame(s.mat_acq, stringsAsFactors = FALSE)
nrow(s.df_acq)
# En la última columna colocaremos el Tema de cada documento tdm[["name"]. Para ello usaremos dos funciones cbind() y rep()  

# Recordemos que en la lista TDM habíamos almacenado el tema en el valor "name"
# Mediante la función rep() repetiremos el tema del documento tantas veces como filas hay en el data.frame
?cbind # Con cbind() combinamos vectores, matrices o df por columnas o filas
Tema <- rep(mat[[1]]$name, nrow(s.df_acq))
s.df_acq<-cbind(s.df_acq,Tema)

# Para earn
s.mat_earn <- t(data.matrix(mat[[2]]$mat))
# La convertimos en data.frame que vendría a ser como un formato excel (filas, columnas y celdas con valores)  
# En este data.frame , tenemos que cada fila es un documento, cada columna una palabra y las celdas contienen la frecuencia en que cada palabra aparece en cada documento.
s.df_earn <- as.data.frame(s.mat_earn, stringsAsFactors = FALSE)
# En la última columna colocaremos el Tema de cada documento tdm[["name"]. Para ello usaremos dos funciones cbind() y rep()  

# Recordemos que en la lista TDM habíamos almacenado el tema en el valor "name"
# Mediante la función rep() repetiremos el tema del documento tantas veces como filas hay en el data.frame 
Tema <- rep(mat[[2]]$name, nrow(s.df_earn))
s.df_earn<-cbind(s.df_earn,Tema)

# Utilizamos la función rbind.fill() para concatenar las filas de dos data frame con distinta dimensión y pone NA en las casillas donde no hay información.
pila <-rbind.fill(s.df_acq, s.df_earn)
pila[is.na(pila)] <- 0

# Cada fila representa un documento, cada columna una palabra y las celdas son la frecuencia de aparición de esa palabra en ese documento.
## Tenemos 4436 documentos 
nrow(pila)
## Tenemos 48 palabras
ncol(pila)
```


******
## Construcción del Modelo de clasificación utilizando la función `knn` del paquete "class"
******
Construimos un **juego de datos de entrenamiento** con el 70% de los documentos, es decir, 3106 documentos.    
Así mismo construiremos un **juego de datos de pruebas** con el 30% de documentos restante, es decir 1330 documentos.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Fijamos una semilla para poder repetir la práctica obteniendo los mismos resultados. 
?set.seed
set.seed(111)

# 70% de los documentos para entrenamiento
entrena.idx <- sample(nrow(pila), ceiling(nrow(pila) * 0.7))
# El resto de documentos para pruebas
test.idx <- (1:nrow(pila))[-entrena.idx]
```

Para poder aplicar el algoritmo de aprendizaje por vecindad K-NN necesitamos realizar unas pequeñas adaptaciones.  
Éstas consisten en separar por un lado los temas y por otro la matriz de frecuencias.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# guardamos por un lado los temas
tema <- pila[, "Tema"]
# y por otro lado el resto de palabras
pila.nl <- pila[, !colnames(pila) %in% "Tema"]
```

**Aplicamos el modelo K-NN**, pasándole como parámetros la matriz de frecuencias de los documentos de entrenamiento, la matriz de frecuencias de los documentos de pruebas y los temas de los documentos de entrenamiento.  

Los temas de los documentos de prueba no se los pasamos, porque precisamente es lo que el algoritmo debe predecir.  

Recordamos que **el objetivo del modelo será el de predecir el tema de los documentos de pruebas**.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Modelo KNN
knn.pred <- knn(pila.nl[entrena.idx, ], pila.nl[test.idx, ], tema[entrena.idx])
```


******
## Construcción del Modelo de clasificación utilizando la función `train` del paquete "caret"
******

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
library(caret)
# Creamos la base de datos de entrenamiento con la dependiente (tema) y las explicativas (palabras)
dat_train<-cbind(pila.nl[entrena.idx, ],tema[entrena.idx])
# Definimos una malla de valores para k
?expand.grid
knnGrid <-  expand.grid(k = c(1:3))
# Modelo KNN
knn.caret <- train(`tema[entrena.idx]`~., data=dat_train,method = "knn",tuneGrid = knnGrid)
knn.caret
```

******
# Validación del Modelo de clasificación
******

Una vez aplicado el modelo K-NN sobre el juego de documentos de prueba, podemos utilizar una **matriz de confusión** para valorar el nivel de acierto del modelo.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Modelo KNN
?knn
knn.pred <- knn(pila.nl[entrena.idx, ], pila.nl[test.idx, ], tema[entrena.idx])
# Matriz de confusión
# Las filas son predicciones y las columnas son observaciones reales
conf.mat <- table("Predicción" = knn.pred,"Real" = tema[test.idx])
conf.mat
```
Observamos como K-NN, de los 1330 documentos, ha clasificado correctamente 1292:  

* 460 documentos como acq. 
* 830 documentos como earn.  

y ha fallado en 40 documentos, puesto que los ha clasificado 10 como acq cuando en realidad eran earn y 30 como earn que en reladad eran acq.  

Para evaluar la capacidad predictiva del algoritmo utilizamos dos medidas, que hemos denominado ratio1 y rati2.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
ratio1 <- (conf.mat[1,1]/(conf.mat[1,1]+conf.mat[2,1]))*100
ratio1
```
```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
ratio2 <- (conf.mat[2,2]/(conf.mat[1,2]+conf.mat[2,2]))*100
ratio2
```

******
## Matriz de confusión con el paquete "caret"
******

Obtenemos una matriz de confusión más completa. Además de la precisión, la sensibilidad y la especificidad, esta matriz de confusión proporciona estadísticos que nos ayudan a valorar la capacidad informativa del modelo.

Es importante tener en cuenta que la categoría sobre la que calcula la probabilidad es la menor, o en caso de alfanuméricas la primera en el alfabeto. En este caso es acq (respecto a earn), esto lo indica al final de la matriz de confusión.

|          |	Reference      	|
|:---------|:------|:---------|
|Predicted |	Event|	No Event|
|Event     |A	     |B         |
|No Event  |C      |D         |


Los resultados son:

Sensitivity = A/(A+C)

Specificity = D/(B+D)

Prevalence = (A+C)/(A+B+C+D)

PPV = (sensitivity * prevalence)/((sensitivity * prevalence) + ((1-specificity) * (1-prevalence)))

NPV = (specificity * (1-prevalence))/(((1-sensitivity) * prevalence) + ((specificity) * (1-prevalence)))

Detection Rate = A/(A+B+C+D)

Detection Prevalence = (A+B)/(A+B+C+D)

Balanced Accuracy = (sensitivity+specificity)/2

Precision = A/(A+B)

Además de lo anterior la matriz de confusión también informa del 

- porcentaje de acierto: Accuracy = (A+D)/(A+B+C+D)

- cociente no informativo (NIR): No Information Rate = (A+C)/(A+B+C+D) = Prevalence

- p-valor para el contraste de la $H_0: Accuracy>NIR$: $P-Value [Acc > NIR]$, es una medida de capacidad informativa del modelo. **Cuanto menor es el p-valor mejor**

- Estadístico Kappa: k= (Accuracy-Random Acc)/(1-Random Acc), donde Random Acc=(p1*p2)+(1-p1)*(1-p2), 
con p1=Prevalence y p2=Detection Prevalence. **Cuanto más próxima a 1 se sitúe el valor de Kappa mayor poder informativo del modelo**.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
dat_test<-cbind(pila.nl[test.idx, ],tema[test.idx])
preds_knn<-predict(knn.caret, dat_test) 
# Matriz de confusión
confusionMatrix(preds_knn,as.factor(dat_test$`tema[test.idx]`)) 
```

******
# Ejercicios
******

## Ejercicio 1:
En el apartado "Generación de la Matriz de Términos", un valor a controlar es la "Sparsity" que se fija en 0.80. Describa como se calcula el valor de la Sparsity que finalmente se consigue en ambas matrices de términos. ¿Qué implica que la "Sparsity" en  la matriz de términos asociada a la temática earn sea menor?.

## Respuesta 1:

> La "Sparsity" o dispersión es el número de celdas igual a 0 respecto del total. En nuestro caso, para la matriz de términos de "acq", antes de fijar la dispersión en 0.8, esta era del 0.99, es decir había términos que no aparecían en el 99% de los documentos. Sólamente 75448 celdas de las 12544124 totales tenían valores distintos de 0. 

>Al fijar la dispersión en 0.8 estamos indicando que la dispersión máxima permitida es del 0.8, es decir como máximo, pueden no aparecer en el 80% de los documentos. Continuando con el ejemplo de "acq", vemos que al fijar un máximo de 0.8 la dispersión baja a 0.63, quedándose con 20 términos de los 7907 iniciales y teniendo las 75448 celdas distintas de 0 de las 20173 con las que nos hemos quedado.
Podemos apreciar que la dispersión resultante, 0.63, al haber fijado una dispersión máxima permitida de 0.80, es considerablemente menor que la fijada. Esto se debe a que al quitar los términos cuya dispersión es mayor o igual que 0.8, los terminos resultantes aparecen en el 37% de los documentos, de modo que la dispersión decrece al 0.63.

## Ejercicio 2:
En el apartado "Visualizaciones sobre la matriz de palabras TDM" se muestran algunos ejemplos de visualización de distintas secciones de la matriz de términos. Visualizad los 10 primeros términos y los 15 primeros documentos en la temática "acq" y todos los términos de los 5 primeros documentos de la temática "earn". Posteriormente visualizar aquellas palabras relacionadas con esta misma temática y con frecuencia menor a 500 y mayor a 5.

## Respuesta 2:
> 

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Frecuencia de los 10 primeros términos en los 15 primeros documentos para la temática "acq"
inspect(mat[[1]]$mat[1:10,1:15])


# Frecuencia de todos los términos de los 5 primeros documentos del tema "earn"
inspect(mat[[2]]$mat[,1:5])

# Visualizamos los términos con más de 5 apariciones y menos de 500 en documentos de temática "earn"
findFreqTerms(mat[[2]]$mat, lowfreq=5, highfreq=500)
```


## Ejercicio 3:
En el apartado "Descripción de la TDM" se construye un gráfico de barras con las palabras con una frecuencia mayor que 500 en ambas temáticas.
Construya un gráfico de barras similar, con las palabras con una frecuencia mayor que 100, con las barras apiladas y en posición horizontal. ¿Qué implica la existencias de estas barras con un solo color?.


## Respuesta 3: 
>Tenemos que fdata es el dataframe formado por tres columnas: Palabra, Frecuencia y Temática ("acq" o "earn"), donde la Frecuencia es el número total de veces que la palabra aparece en todos los documentos de la temática x.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Gráfico de barras con las palabras con frecuencia mayor que 100 de ambas temáticas
library(ggplot2)
?ggplot
?geom_bar
# Barras es vertical
ggplot(subset(fdata, Frecuencia>100),aes(Palabra,Frecuencia,fill=Tematica))+geom_bar(stat="identity",position=position_dodge())+theme(axis.text.x=element_text(angle=45, hjust=1))
# Barras en horizontal
ggplot(subset(fdata, Frecuencia>100),aes(Palabra,Frecuencia,fill=Tematica))+geom_bar(stat="identity",position=position_dodge())+theme(axis.text.x=element_text(angle=45, hjust=1))+ coord_flip()
# Barras en horizontal y apiladas
ggplot(subset(fdata, Frecuencia>100),aes(Palabra,Frecuencia,fill=Tematica))+geom_bar(stat="identity")+theme(axis.text.x=element_text(angle=45, hjust=1))+ coord_flip()
```
> Que para ciertas palabras no aparezcan barras de ambos colores quiere decir que dicha palabra aparece en documentos de una sola temática. Por ejemplo, para el caso year, sólo aparece la barra azul, lo cual quiere decir que esta palabra sólo aparece en documentos de la temática "earn".

## Ejercicio 4: 
Se ha construido un modelo de clasificación con la función `knn()` y con la función `train()`. Describa las diferencias que hay en el tipo de entrenamiento del modelo realizado con cada función.

## Respuesta 4: 

> De cara a aplicar la función 'knn()' consideramos un dataframe "pila" en el que las filas son documentos y las columnas son las palabras, de modo que cada celda representa la frecuencia de aparición de cada palabra en cada documento; además tiene una última fila "Tema" que indica el tema al que pertenece el documento ("adq" o "earn"). Ahora bien, fijamos el 70% de los indices de las filas del dataframe "pila" y lo denotamos "entrena.idx", ya que será nuestro conjunto de entrenamiento, mientras que el 30% restante, "test.idx", será el conjunto de prueba.
Por último, guardamos por un lado la columna "Tema" en el df "temas" y por otro guardamos el resto de las columnas en el df "pila.nl".
De este modo, a la función 'knn()' le pasaremos como parámetros la matriz de frecuencias de los documentos de entrenamiento, "pila.nl[entrena.idx,]"; la matriz de frecuencias de los documentos de pruebas, "pila.nl[test.idx, ]" y los temas de los documentos de entrenamiento, "tema[entrena.idx]".
Cabe destacar que, por defecto, la función 'knn()' considera k = 1, i.e. siempre buscará al vecino más cercano, y lo hará en base a la distacia Euclídea. De este modo, para cada fila del conjunto de prueba buscará al vecino más cercano de entre las filas del conjunto de entrenamiento y devolverá el tema de ese (a menos que determinemos que k tiene otro valor).

> Para el caso de aplicar la función "train()" consideraremos el dataframe "dat_train", resultante de unir las filas del conjunto de entrenamiento del dataframe "pila.nl", i.e. la matriz de frecuencias de los documentos de entrenamiento, "pila.nl[entrena.idx,]" , y los temas de los documentos de entrenamiento, "tema[entrena.idx]".
Le pasamos entonces dicho dataframe, le indicamos mediante "`tema[entrena.idx]`~." que utilice todos los atributos en nuestra clasificación y "tema[entrena.idx,]" como nuestra variable objetivo, y por último, que el método que deseamos utlizar es el knn.

> La principal diferencia que puedo extraer es que para el caso de "train()" los parámetros que se pasan son sólamente los de entrenamiento, junto con los posibles valores de k y con todo ello define el modelo, pero sin aplicarlo. Así, para extraer las predicciones para el conjunto de prueba debemos pasar lo que esta nos devuelva por "predict()". Mientras que con la función "knn()" le pasamos tanto el conjunto de entrenamiento como el de prueba, además de la k concreta que vayamos a utilizar, y ella misma nos devuelve las predicciones para el de prueba, i.e. aplica el modelo de forma directa.

## Ejercicio 5: 
En el apartado "Validación del Modelo de clasificación" se utiliza la función "knn" para clasificar los documentos según temática. Esta función por defecto supone que el número de vecinos a evaluar es igual a 1. ¿Qué dificultad añadida podría tener seleccionar un número de vecinos igual a 2?.   

## Respuesta 5: 

> Como indiqué en la pregunta anterior, el modelo de clasificación knn, a la hora de clasificar un documento del conjunto de prueba buscará los k (por defecto 1) vecinos más cercanos en base a la distancia Euclídea. De este modo, para cada fila del conjunto de prueba buscará los k vecinos más cercanos de entre las filas del conjunto de entrenamiento y devolverá el tema de la mayoría de los resultante. Ahora bien, en caso de que k = 2 y que haya dos filas, i.e. documentos, cuya distancia Euclídea a nuestro documento de prueba sea la menor de entre todas las demás y tengan temas diferentes, entonces no podremos determinar el tema de nuestro documento.

## Ejercicio 6: 
Del fichero inicial seleccionar dos temáticas distintas a las del enunciado y entrenar un k-NN utilizando la función `train()` que permita clasificar nuevos artículos en una de ambas temáticas. En la función `train()` defina una malla de valores para K impares entre 1 y 15, comente cuál es el valor óptimo y describa detalladamente como se realizaría una predicción de un nuevo dato.


## Respuesta 6: 

Comenzamos eligiendo la temática.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Seleccionamos dos temáticas: crude y trade, que son las segundas más representativas, como se puede apreciar en el gráfico de frecuencias del comienzo.
data3<-data[which(data$V1 %in% c("crude","trade")),]
## Cuantos hay en total
nrow(data3)
```

A continuación procesamos los datos.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Creación del corpus de la temática crude
## Seleccionamos la temática
data_crude<-data3[(data3$V1=="crude"),]
## Construimos el corpus
### A vector source interprets each element of the vector data_acq$V2 as a document.
?VectorSource
source1 <- VectorSource(data_crude$V2)
### Corpora are collections of documents containing (natural language) text.
?Corpus
corpus1 <- Corpus(source1)
## Acondicionamos el corpus
?tm_map
### Interface to apply transformation functions (also denoted as mappings) to corpora.
### Convertir todo el texto a minúsculas
corpus1 <- tm_map(corpus1, content_transformer(tolower))
### Elimina números
corpus1 <- tm_map(corpus1, removeNumbers)
### Eliminar signos de puntuación
corpus1 <- tm_map(corpus1, removePunctuation)
###Eliminar espacios en blanco innecesarios
corpus1 <- tm_map(corpus1, stripWhitespace)
### Eliminar palabras sin significado propio
v_stopwords <- c(stopwords("english"),c("dont","didnt","arent","cant","one","also","said"))
corpus1 <- tm_map(corpus1, removeWords, v_stopwords)
### Eliminar signos de puntuación
corpus1 <- tm_map(corpus1, removePunctuation)
### Substituir las palabras derivadas por su palabra raíz
corpus1 <- tm_map(corpus1, stemDocument, language="english")

# Creación del corpus de la temática trade
## Seleccionamos la temática
data_trade<-data3[(data3$V1=="trade"),]
## Construimos el corpus
source1 <- VectorSource(data_trade$V2)
corpus2 <- Corpus(source1)
## Acondicionamos el corpus
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, stripWhitespace)
v_stopwords <- c(stopwords("english"),c("dont","didnt","arent","cant","one","also","said"))
corpus2 <- tm_map(corpus2, removeWords, v_stopwords)
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, stemDocument, language="english")
```

Generamos la matriz de términos de cada temática.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Construimos la matrix de documentos de la temática crude
?TermDocumentMatrix
mat_crude <- TermDocumentMatrix(corpus1)
mat_crude
## Controlamos la dispersión (Sparsity): Número de celdas igual a cero respecto al total.
?removeSparseTerms
mat_crude<- removeSparseTerms(mat_crude,  0.80)
### Para poder ver la matriz utilizamos inspect
inspect(mat_crude)
### Crea una lista: el primer elemento es name y el segundo es la matriz mat_crude, a la que llama mat
?list
mat_crude<-list(name="crude",mat=mat_crude)
mat_crude
str(mat_crude)

# Construimos la matrix de documentos de la temática trade
mat_trade <- TermDocumentMatrix(corpus2)
mat_trade<- removeSparseTerms(mat_trade,  0.80)
inspect(mat_trade)
mat_trade<-list(name="earn",mat=mat_trade)
mat_trade
str(mat_trade)

# Juntamos ambas matrices de términos en una misma lista
mat<-list(mat_crude, mat_trade)
str(mat)
```

Representamos la gráfica de frecuencias.

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Para crude
mmat_crude <- as.matrix(mat[[1]]$mat)
# Agregamos las frecuencias por términos y las ordenamos de mayor a menor
v_crude <- sort(rowSums(mmat_crude), decreasing=TRUE)
# Creamos un data.frame con términos y frecuencias
d_crude <- data.frame(word=names(v_crude), freq=v_crude)
d_crude[,3]<-"crude"
# Hacemos lo mismo para trade
mmat_trade <- as.matrix(mat[[2]]$mat)
# Agregamos las frecuencias por términos y las ordenamos de mayor a menor  
v_trade <- sort(rowSums(mmat_trade), decreasing=TRUE)
# Creamos un data.frame con términos y frecuencias
d_trade <- data.frame(word=names(v_trade), freq=v_trade)
d_trade[,3]<-"earn"

# Concatenamos las dos matrices
fdata<-rbind(d_crude,d_trade)
colnames(fdata)
colnames(fdata)<-c("Palabra", "Frecuencia", "Tematica")

# Gráfico de barras con las palabras más frecuentes
library(ggplot2)

ggplot(subset(fdata, Frecuencia>500),aes(Palabra,Frecuencia,fill=Tematica))+geom_bar(stat="identity",position=position_dodge())+theme(axis.text.x=element_text(angle=45, hjust=1))
```

A continuación se construirá un data.frame en el que las columnas representan Términos, las filas Documentos y las celdas Frecuencias del término o palabra en cada documento.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Creación de un data.frame apto para K-NN
# Para crude
# La función t transpone la matriz o el df dado
s.mat_crude <- t(data.matrix(mat[[1]]$mat))
# La convertimos en data.frame que vendría a ser como un formato excel (filas, columnas y celdas con valores)  
# En este data.frame, tenemos que cada fila es un documento, cada columna una palabra y las celdas contienen la frecuencia en que cada palabra aparece en cada documento.
s.df_crude <- as.data.frame(s.mat_crude, stringsAsFactors = FALSE)
nrow(s.df_crude)
# En la última columna colocaremos el Tema de cada documento tdm[["name"]. Para ello usaremos dos funciones cbind() y rep()  

# Recordemos que en la lista TDM habíamos almacenado el tema en el valor "name"
# Mediante la función rep() repetiremos el tema del documento tantas veces como filas hay en el data.frame
?cbind # Con cbind() combinamos vectores, matrices o df por columnas o filas
Tema <- rep(mat[[1]]$name, nrow(s.df_crude))
s.df_crude<-cbind(s.df_crude,Tema)

# Para trade
s.mat_trade <- t(data.matrix(mat[[2]]$mat))
# La convertimos en data.frame que vendría a ser como un formato excel (filas, columnas y celdas con valores)  
# En este data.frame , tenemos que cada fila es un documento, cada columna una palabra y las celdas contienen la frecuencia en que cada palabra aparece en cada documento.
s.df_trade <- as.data.frame(s.mat_trade, stringsAsFactors = FALSE)
# En la última columna colocaremos el Tema de cada documento tdm[["name"]. Para ello usaremos dos funciones cbind() y rep()  

# Recordemos que en la lista TDM habíamos almacenado el tema en el valor "name"
# Mediante la función rep() repetiremos el tema del documento tantas veces como filas hay en el data.frame 
Tema <- rep(mat[[2]]$name, nrow(s.df_trade))
s.df_trade<-cbind(s.df_trade,Tema)

# Utilizamos la función rbind.fill() para concatenar las filas de dos data frame con distinta dimensión y pone NA en las casillas donde no hay información.
pila <-rbind.fill(s.df_crude, s.df_trade)
pila[is.na(pila)] <- 0

# Cada fila representa un documento, cada columna una palabra y las celdas son la frecuencia de aparición de esa palabra en ese documento.
## Tenemos 4436 documentos 
nrow(pila)
## Tenemos 48 palabras
ncol(pila)
```

Construimos un **juego de datos de entrenamiento** con el 70% de los documentos, es decir, 3106 documentos.    
Así mismo construiremos un **juego de datos de pruebas** con el 30% de documentos restante, es decir 1330 documentos.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# Fijamos una semilla para poder repetir la práctica obteniendo los mismos resultados. 
?set.seed
set.seed(111)

# 70% de los documentos para entrenamiento
entrena.idx <- sample(nrow(pila), ceiling(nrow(pila) * 0.7))
# El resto de documentos para pruebas
test.idx <- (1:nrow(pila))[-entrena.idx]
```

Para poder aplicar el algoritmo de aprendizaje por vecindad K-NN necesitamos realizar unas pequeñas adaptaciones.  
Éstas consisten en separar por un lado los temas y por otro la matriz de frecuencias.  

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
# guardamos por un lado los temas
tema <- pila[, "Tema"]
# y por otro lado el resto de palabras
pila.nl <- pila[, !colnames(pila) %in% "Tema"]
```

A continuación construimos el Modelo de clasificación utilizando la función `train` del paquete "caret"

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
library(caret)
# Creamos la base de datos de entrenamiento con la dependiente (tema) y las explicativas (palabras)
dat_train<-cbind(pila.nl[entrena.idx, ],tema[entrena.idx])
# Definimos una malla de valores para k
knnGrid <-  expand.grid(k = seq(1, 15, 2))
# Modelo KNN
knn.caret <- train(`tema[entrena.idx]`~., data=dat_train,method = "knn",tuneGrid = knnGrid)
knn.caret
```

Por último, obtenemos la matriz de confusión para el modelo caret:

```{r,eval=TRUE,echo=TRUE,warning=FALSE, message=FALSE}
dat_test<-cbind(pila.nl[test.idx, ],tema[test.idx])
preds_knn<-predict(knn.caret, dat_test) 
# Matriz de confusión
confusionMatrix(preds_knn,as.factor(dat_test$`tema[test.idx]`)) 
```


## Ejercicio 7: 
¿En qué distancia se basa el algoritmo K-NN que se define en la función knn() de R que se ha utilizado para entrenar el modelo?. Realizad una o más críticas a dicha distancia y describid cómo podría corregirse.


## Respuesta 7: 

> La distancia que utiliza la función knn() es la distancia Euclídea, que como bien expresa Jordi Gironés en su libro 'Business Analytics. Análisis de datos para organizaciones': utilizar esta distancia presenta el problema de que no tiene en cuenta las diferentes unidades de medida en las que pueden estar expresadas las variables entre las cuáles estamos calculando la distancia. En nuestro caso, ambas variables expresan la frecuencia de aparición de ciertas palabras en dos documentos diferentes, es decir, la unidad de medida es la misma, pero puede darse el caso de que una de las palabras aparezca un número muy elevado de veces, junto con muchas otras en un documento, mientras en otro documento puede haber el mismo número de palabras de todas ellas, menos de la primera, que aparece en menos medida y eso haga que la distancia crezca, cuando en realidad ambos documentos tendrán la misma temática posiblemente.

> Para solventar el problema causado por las diferentes unidades de medida utilizadas en las variables entre las cuales se quiere medir la distancia surge la distancia estadística o de Gauss, que "normaliza" las variables de manera que se presenten en la misma escala.

## Ejercicio 8: 

¿Afirmaría que en todos los modelos entrenados en los ejercicios de la práctica el algoritmo KNN es determinista?

## Respuesta 8: 

> Un algoritmo determinista es un algoritmo que está puramente determinado por sus entradas, donde no hay aleatoriedad en el modelo. Los algoritmos deterministas siempre obtendrán el mismo resultado con las mismas entradas.
Por la definición, parece claro que los modelos que hemos entrenado son deterministas, ya que hemos fijado los valores que forman el conjunto de entrenamiento, de modo que el cálculo de la distacia siempre se hará con respecto a ellos. Por el contrario, si cada vez que utilizasemos el modelo el conjunto de entrenamiento cambiase, entonces los resultados variarían en cada una de las aplicaciones.